{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install tensorflow\n",
    "# %pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMODITY = 'magnesium'\n",
    "\n",
    "DATE_COLUMN = 'Date'\n",
    "VALUE_COLUMN = 'Value'  \n",
    "QUANTITY_COLUMN = 'Std. Quantity (KG)'\n",
    "UNIT_RATE_COLUMN = 'Std. Unit Rate ($/KG)'\n",
    "BRENT_OIL_COLUMN = 'Brent Oil Value'\n",
    "WTI_OIL_COLUMN = 'WTI Oil Value'\n",
    "\n",
    "VALUE_SPIKES_COLUMN = 'Value Spikes'  \n",
    "QUANTITY_SPIKES_COLUMN = 'Std. Quantity (KG) Spikes'\n",
    "UNIT_RATE_SPIKES_COLUMN = 'Std. Unit Rate ($/KG) Spikes'\n",
    "BRENT_OIL_SPIKES_COLUMN = 'Brent Oil Value'\n",
    "WTI_OIL_SPIKES_COLUMN = 'WTI Oil Value'\n",
    "\n",
    "ORIGIN_COUNTRY_COLUMN = 'Country of Origin'\n",
    "DEST_COUNTRY_COLUMN = 'Country of Destination'\n",
    "\n",
    "PETROL_FILE_PATH = '../volza/petroleum/petrol_crude_oil_spot_price.csv'\n",
    "VOLZA_FILE_PATH = '../volza/magnesium/magnesium.csv'\n",
    "PRICE_FILE_PATH = \"../volza/magnesium/magnesium_price_2.csv\"\n",
    "\n",
    "SPIKES_THRESHOLD = 2\n",
    "SPIKES_WINDOW_SIZE = 20\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows where we have usable quantity units (kg, ton) and standardizing it.\n",
    "def convert_to_kg(df, quantity_col='Std. Quantity', unit_col='Std. Unit'):\n",
    "    converstion_factors = {\n",
    "        'TON': 907.185,\n",
    "        'TNE': 1000,\n",
    "        'KGS': 1,\n",
    "        'Kgs': 1,\n",
    "    }\n",
    "\n",
    "    df_filtered = df[df[unit_col].isin(converstion_factors.keys())]\n",
    "\n",
    "    def convert(row):\n",
    "        unit = row[unit_col]\n",
    "        quantity = row[quantity_col]\n",
    "        return quantity * converstion_factors.get(unit,1)\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered[VALUE_COLUMN] != 0]\n",
    "    df_filtered[QUANTITY_COLUMN] = df_filtered.apply(convert, axis=1)\n",
    "    df_filtered = df_filtered[df_filtered[QUANTITY_COLUMN] != 0]\n",
    "\n",
    "    df_filtered[UNIT_RATE_COLUMN] = df_filtered[VALUE_COLUMN] / df_filtered[QUANTITY_COLUMN]\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#Formatting the date and price for Volza data\n",
    "volza_pd = pd.read_csv(VOLZA_FILE_PATH)\n",
    "volza_pd = volza_pd[(volza_pd[\"Country of Origin\"].notnull()) & (volza_pd[\"Country of Destination\"].notnull())]\n",
    "volza_pd = volza_pd.rename(columns={'Unnamed: 0': 'ID'})\n",
    "volza_pd['Date'] = volza_pd['Date'].apply(lambda x: x.split(' ')[0])\n",
    "volza_pd['Date'] = pd.to_datetime(volza_pd['Date'], errors='raise', format='%Y-%m-%d')\n",
    "volza_pd = convert_to_kg(volza_pd)\n",
    "volza_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing the price data\n",
    "prices_pd = pd.read_csv(PRICE_FILE_PATH)\n",
    "prices_pd['Date'] = prices_pd['Date'].apply(lambda x: datetime.strptime(x, \"%b %d, %Y\").strftime(\"%Y-%m-%d\"))\n",
    "prices_pd['Date'] = pd.to_datetime(prices_pd['Date'], errors='raise', format='%Y-%m-%d')\n",
    "prices_pd['Price'] = prices_pd['Price'].str.replace(',', '').astype(float)\n",
    "prices_pd = prices_pd[['Date','Price']]\n",
    "prices_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate volza data by day\n",
    "date_wise_volza = volza_pd.groupby(\"Date\")[[VALUE_COLUMN,QUANTITY_COLUMN,'Gross Weight']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg of Commodity Price in Volza\n",
    "avg_price_volza = volza_pd.groupby('Date')[UNIT_RATE_COLUMN].mean()\n",
    "date_wise_volza = date_wise_volza.join(avg_price_volza, how='left')\n",
    "date_wise_volza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Petroleum data prep\n",
    "petrol_df = pd.read_csv(PETROL_FILE_PATH, delimiter=';', on_bad_lines='warn')\n",
    "petrol_df['Date'] = pd.to_datetime(petrol_df['Date'])\n",
    "\n",
    "# Split based on types of oil\n",
    "brent_df = petrol_df[petrol_df['product-name']=='UK Brent Crude Oil']\n",
    "wti_df = petrol_df[petrol_df['product-name']=='WTI Crude Oil']\n",
    "\n",
    "brent_df.rename(columns={'Value':'Brent Oil Value'}, inplace=True)\n",
    "wti_df.rename(columns={'Value':'WTI Oil Value'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining dataframes\n",
    "prices_pd = prices_pd.set_index('Date')\n",
    "aggregated_df = date_wise_volza.join(prices_pd, how=\"left\").fillna(method='ffill')\n",
    "aggregated_df = aggregated_df.merge(brent_df[[DATE_COLUMN, BRENT_OIL_COLUMN]], on='Date', how='left').fillna(method='ffill')\n",
    "aggregated_df = aggregated_df.merge(wti_df[[DATE_COLUMN, WTI_OIL_COLUMN]], on='Date', how='left').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_spikes(df, column):\n",
    "    ## Detecting spikes\n",
    "    moving_avg = df[column].rolling(window=SPIKES_WINDOW_SIZE).mean()\n",
    "    std_dev = df[column].rolling(window=SPIKES_WINDOW_SIZE).std()\n",
    "\n",
    "    # Set a threshold to identify spikes\n",
    "    return (abs(aggregated_df[column] - moving_avg) > SPIKES_THRESHOLD * std_dev).astype(int)\n",
    "\n",
    "aggregated_df['spikes'] = detect_spikes(aggregated_df, 'Price')\n",
    "print(\"SPIKES : NON SPIKES = \")\n",
    "print(aggregated_df['spikes'].value_counts())\n",
    "print(\"PERCENT OF SPIKES\", aggregated_df['spikes'].value_counts()[1]/len(aggregated_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Detect spikes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df[VALUE_SPIKES_COLUMN] = detect_spikes(aggregated_df, VALUE_COLUMN)\n",
    "aggregated_df[QUANTITY_SPIKES_COLUMN] = detect_spikes(aggregated_df, QUANTITY_COLUMN)\n",
    "aggregated_df[UNIT_RATE_SPIKES_COLUMN] = detect_spikes(aggregated_df, UNIT_RATE_COLUMN)\n",
    "aggregated_df[WTI_OIL_SPIKES_COLUMN] = detect_spikes(aggregated_df, WTI_OIL_COLUMN)\n",
    "aggregated_df[BRENT_OIL_SPIKES_COLUMN] = detect_spikes(aggregated_df, BRENT_OIL_COLUMN)\n",
    "\n",
    "#Visualise Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting the graph\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plotting 'Value', 'Quantity', and 'Gross Weight' on the left y-axis\n",
    "ax1.plot(aggregated_df.index, aggregated_df[VALUE_SPIKES_COLUMN], label='Value Spikes', color='b')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[QUANTITY_SPIKES_COLUMN], label='Quantity Spikes', color='g')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[UNIT_RATE_SPIKES_COLUMN], label='Unit Rate Spikes', color='k')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[BRENT_OIL_SPIKES_COLUMN], label='Brent Oil Value Spikes', color='m')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[WTI_OIL_SPIKES_COLUMN], label='WTI Oil Value Spikes', color='c')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Value / Quantity / Gross Weight', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Creating a second y-axis for 'Price'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(aggregated_df.index, aggregated_df['Price'], label='Price', color='orange')\n",
    "ax2.set_ylabel('Price', color='orange')\n",
    "ax2.tick_params('y', colors='orange')\n",
    "\n",
    "# Display legend\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove date 2020-01-01\n",
    "aggregated_df = aggregated_df[aggregated_df.index != '2020-01-01']\n",
    "aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting the graph\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plotting 'Value', 'Quantity', and 'Gross Weight' on the left y-axis\n",
    "ax1.plot(aggregated_df.index, aggregated_df[VALUE_COLUMN], label='Value', color='b')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[QUANTITY_COLUMN], label='Quantity', color='g')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[UNIT_RATE_COLUMN], label='Unit Rate', color='k')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[BRENT_OIL_COLUMN], label='Brent Oil Value', color='m')\n",
    "ax1.plot(aggregated_df.index, aggregated_df[WTI_OIL_COLUMN], label='WTI Oil Value', color='c')\n",
    "ax1.plot(aggregated_df.index, aggregated_df['Gross Weight'], label='Gross Weight', color='r')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Value / Quantity / Gross Weight', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Creating a second y-axis for 'Price'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(aggregated_df.index, aggregated_df['Price'], label='Price', color='orange')\n",
    "ax2.set_ylabel('Price', color='orange')\n",
    "ax2.tick_params('y', colors='orange')\n",
    "\n",
    "# Display legend\n",
    "fig.tight_layout()\n",
    "fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.9))\n",
    "\n",
    "# Display the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the price data\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(aggregated_df.index, aggregated_df['Price'], label='Price', color='blue')\n",
    "\n",
    "# Highlighting spikes\n",
    "spike_indices = aggregated_df[aggregated_df['spikes'] == 1].index\n",
    "spike_prices = aggregated_df.loc[spike_indices, 'Price']\n",
    "plt.scatter(spike_indices, spike_prices, color='red', marker='^', label='Spikes')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price Data with Spikes')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count % of spikes \n",
    "total_spikes = aggregated_df['spikes'].sum()\n",
    "total_data_points = len(aggregated_df)\n",
    "percentage_of_spikes = (total_spikes / total_data_points) * 100\n",
    "\n",
    "print(f\"Percentage of Spikes: {percentage_of_spikes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Probability of spike\n",
    "spike_prob = aggregated_df['spikes'].mean()\n",
    "\n",
    "# Random baseline predictions\n",
    "random_predictions = np.random.choice([0, 1], size=len(aggregated_df), p=[1-spike_prob, spike_prob])\n",
    "\n",
    "# Calculate precision and recall for the random baseline\n",
    "random_precision = precision_score(aggregated_df['spikes'], random_predictions)\n",
    "random_recall = recall_score(aggregated_df['spikes'], random_predictions)\n",
    "\n",
    "print(f\"Random Guessing Precision: {random_precision}\")\n",
    "print(f\"Random Guessing Recall: {random_recall}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discretize\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten, SimpleRNN\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Attention, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Attention, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [VALUE_COLUMN, QUANTITY_COLUMN, UNIT_RATE_COLUMN, WTI_OIL_COLUMN, BRENT_OIL_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 12ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:219: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  centers = km.fit(column[:, None]).cluster_centers_[:, 0]\n",
      "c:\\Users\\nocet\\OneDrive\\Documents\\Projects\\lab-v2\\ezra\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  \"decreasing the number of bins.\" % jj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 14ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "6/6 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for feature in FEATURES:\n",
    "    feature_name = feature.replace('/', '')\n",
    "    FEATURE_COLUMNS = FEATURES.copy()\n",
    "    FEATURE_COLUMNS.remove(feature)\n",
    "\n",
    "    def discretize(df, columns, bins):\n",
    "        est = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n",
    "        return est.fit_transform(df[columns])\n",
    "\n",
    "    test_df = aggregated_df.copy()  # Assuming aggregated_df is your DataFrame\n",
    "    discretized_df = discretize(test_df[FEATURE_COLUMNS], FEATURE_COLUMNS, 5)\n",
    "    test_df[FEATURE_COLUMNS] = discretized_df\n",
    "    test_df.head(2)\n",
    "    # Convert the discretized data into a DataFrame\n",
    "    discretized_df = pd.DataFrame(discretized_df, columns=FEATURE_COLUMNS)\n",
    "\n",
    "    # Count the frequencies of each bin for each feature\n",
    "    bin_counts = discretized_df.apply(pd.Series.value_counts).fillna(0).T\n",
    "\n",
    "    time_series_df = aggregated_df.copy()\n",
    "\n",
    "    # Drop rows with NaN in the 'spikes' column\n",
    "    time_series_df = time_series_df.dropna(subset=['spikes'])\n",
    "    discretized_df = discretize(time_series_df[FEATURE_COLUMNS], FEATURE_COLUMNS, 5)\n",
    "    time_series_df[FEATURE_COLUMNS] = discretized_df\n",
    "\n",
    "    # Extract features and target variable\n",
    "    X = time_series_df[FEATURE_COLUMNS].values\n",
    "    y = time_series_df['spikes'].values\n",
    "\n",
    "    # Feature scaling using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Create sequences for each sample with a window size of 20\n",
    "    SPIKES_WINDOW_SIZE = 20\n",
    "    X_sequences, y_sequences = [], []\n",
    "\n",
    "    for i in range(len(X_scaled) - SPIKES_WINDOW_SIZE + 1):\n",
    "        X_sequences.append(X_scaled[i:i + SPIKES_WINDOW_SIZE, :])\n",
    "        y_sequences.append(y[i + SPIKES_WINDOW_SIZE - 1])\n",
    "\n",
    "    X_sequences, y_sequences = np.array(X_sequences), np.array(y_sequences)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=50)\n",
    "    def make_output_dict(name, params, classification_report):\n",
    "        return {\n",
    "            \"Name\": name,\n",
    "            \"Params\": params,\n",
    "            \"Accuracy\": classification_report[\"accuracy\"],\n",
    "            \"Precision (0)\": classification_report[\"0\"][\"precision\"],\n",
    "            \"Recall (0)\": classification_report[\"0\"][\"recall\"],\n",
    "            \"F1 (0)\": classification_report[\"0\"][\"f1-score\"],\n",
    "            \"Precision (1)\": classification_report[\"1\"][\"precision\"],\n",
    "            \"Recall (1)\": classification_report[\"1\"][\"recall\"],\n",
    "            \"F1 (1)\": classification_report[\"1\"][\"f1-score\"],\n",
    "        }\n",
    "\n",
    "    output_dicts = []\n",
    "    #LSTM Model\n",
    "    def evaluate_lstm(num_layers: int):\n",
    "        # Build the LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(num_layers, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=False)\n",
    "\n",
    "\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "        return make_output_dict(f\"LSTM\", f\"{num_layers} layers\", classification_report(y_test, y_pred, output_dict=True))\n",
    "\n",
    "\n",
    "    def evaluate_rnn(num_units: int):\n",
    "        # Build the RNN model\n",
    "        model = Sequential()\n",
    "        model.add(SimpleRNN(num_units, input_shape=(X_train.shape[1], X_train.shape[2]), activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=False)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "        # Generate classification report\n",
    "        return make_output_dict(\"RNN\", f\"{num_units} units\", classification_report(y_test, y_pred, output_dict=True))\n",
    "    def evaluate_cnn(num_filters: int, kernel_size: int):\n",
    "        # Build the CNN model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=False)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "        # Generate classification report\n",
    "        return make_output_dict(\"CNN\", f\"{num_filters} filters, kernel size {kernel_size}\", classification_report(y_test, y_pred, output_dict=True))\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_random_forest(n_estimators):\n",
    "        # Create a Random Forest Classifier\n",
    "        random_forest = RandomForestClassifier(n_estimators=10, random_state=213)\n",
    "\n",
    "        X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the classifier\n",
    "        random_forest.fit(X_train_reshaped, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = random_forest.predict(X_test_reshaped)\n",
    "\n",
    "        return make_output_dict(\"Random Forest\", f\"{n_estimators} Estimators\", classification_report(y_test, y_pred, output_dict=True))\n",
    "\n",
    "    def create_attention_cnn_model(input_shape, num_classes, filters, kernel_size):\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "        # CNN layers\n",
    "        conv1 = Conv1D(filters, kernel_size=kernel_size, activation='relu')(inputs)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "        # Reshape for attention\n",
    "        reshape = Reshape((-1, 64))(pool1)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = Attention()([reshape, reshape])\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        flatten = Flatten()(attention)\n",
    "\n",
    "        # Fully connected layers\n",
    "        dense1 = Dense(128, activation='relu')(flatten)\n",
    "        dropout = Dropout(0.5)(dense1)\n",
    "        outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "\n",
    "        # Create model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_attention_cnn(filters, kernel_size):\n",
    "        model = create_attention_cnn_model(X_train.shape[1:], 2, filters, kernel_size)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=filters, verbose=False)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return make_output_dict(\"CNN with Attention\", f\"{filters} filters, kernel size {kernel_size}\", classification_report(y_test, y_pred.argmax(axis=1), output_dict=True))\n",
    "\n",
    "\n",
    "    def create_cnn_model(input_shape, num_classes):\n",
    "        inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "        # CNN layers\n",
    "        conv1 = Conv1D(32, kernel_size=3, activation='relu')(inputs)\n",
    "        pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "        conv2 = Conv1D(64, kernel_size=3, activation='relu')(pool1)\n",
    "        pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "        # Reshape for attention\n",
    "        reshape = Reshape((-1, 64))(pool2)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attention = Attention()([reshape, reshape])\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        flatten = Flatten()(attention)\n",
    "\n",
    "        # Fully connected layers\n",
    "        dense1 = Dense(128, activation='relu')(flatten)\n",
    "        dropout = Dropout(0.5)(dense1)\n",
    "        outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "\n",
    "        # Create model\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def evaluate_all_models(output_file_name):\n",
    "        output_dicts = []\n",
    "\n",
    "        output_dicts.append(evaluate_lstm(100))\n",
    "\n",
    "        # output_dicts.append(evaluate_random_forest(50))\n",
    "        # output_dicts.append(evaluate_random_forest(25))\n",
    "        # output_dicts.append(evaluate_random_forest(20))\n",
    "        # output_dicts.append(evaluate_random_forest(10))\n",
    "        # output_dicts.append(evaluate_random_forest(5))\n",
    "\n",
    "        # output_dicts.append(evaluate_rnn(200))\n",
    "        # output_dicts.append(evaluate_rnn(150))\n",
    "        output_dicts.append(evaluate_rnn(100))\n",
    "        output_dicts.append(evaluate_rnn(50))\n",
    "\n",
    "        # output_dicts.append(evaluate_cnn(64, 3))\n",
    "        output_dicts.append(evaluate_cnn(128, 3))\n",
    "        # output_dicts.append(evaluate_cnn(256, 3))\n",
    "        # output_dicts.append(evaluate_cnn(64, 5))\n",
    "        output_dicts.append(evaluate_cnn(128, 5))\n",
    "        # output_dicts.append(evaluate_cnn(256, 5))\n",
    "\n",
    "        \n",
    "        output_dicts.append(evaluate_attention_cnn(128, 5))\n",
    "\n",
    "        output_dicts = pd.DataFrame(output_dicts)\n",
    "        output_dicts.to_csv(output_file_name)\n",
    "        output_dicts\n",
    "\n",
    "    evaluate_all_models(f\"ablation/{COMMODITY}_model_performance (No Balancing, Ablation {feature_name}).csv\")\n",
    "    # X_train_previous = X_train.reshape(X_train.shape[0], -1)\n",
    "    # y_train_previous = y_train\n",
    "    # X_test_previous = X_test.reshape(X_test.shape[0], -1)\n",
    "    # y_test_previous = y_test\n",
    "\n",
    "    # random_under_sampler = RandomUnderSampler(random_state=RANDOM_STATE)\n",
    "    # X_train, y_train = random_under_sampler.fit_resample(X_train_previous, y_train_previous)\n",
    "\n",
    "    # evaluate_all_models(f\"ablation/{COMMODITY}_model_performance (Random Under Sampling, Ablate {feature_name}).csv\")\n",
    "\n",
    "\n",
    "    # random_under_sampler = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "    # X_train, y_train = random_under_sampler.fit_resample(X_train_previous, y_train_previous)\n",
    "\n",
    "    # evaluate_all_models(f\"ablation/{COMMODITY}_model_performance (Random Over Sampling, Ablate {feature_name}).csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
